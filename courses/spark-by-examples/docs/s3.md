# S3

We'll need the following dependecies, see [Dependencies.scala](../../../project/Dependencies.scala):
- hadoop-common
- hadoop-client
- hadoop-aws

And then configure "spark context" with your AWS credentials:

```scala
val spark: SparkSession = SparkSession.builder()
    .master("local[1]")
    .appName("s3-demo")
    .getOrCreate()

// Replace Key with your AWS account key (You can find this on IAM 
spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", "awsaccesskey value")

// Replace Key with your AWS secret key (You can find this on IAM 
spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", "aws secretkey value")

spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
```

We can read a single text file, multiple files and all files from a directory located on S3 bucket into Spark RDD.

Create a bucket in S3 - I've named mine **csv-backwards** - and upload files from [csv-backwards](../src/main/resources/csv-backwards).

We can then access this bucket in various ways e.g. see [S3App](../src/main/scala/com/backwards/spark/S3App.scala).